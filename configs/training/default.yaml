output_dir: outputs/models
num_epochs: 3
batch_size: 4
eval_batch_size: 8
gradient_accumulation_steps: 4
warmup_steps: 100
learning_rate: 2e-4
weight_decay: 0.01
optimizer: adamw_torch
lr_scheduler_type: cosine
max_grad_norm: 0.3
fp16: false
bf16: true
gradient_checkpointing: true
logging_steps: 10
eval_strategy: steps
eval_steps: 100
save_strategy: steps
save_steps: 500
save_total_limit: 3
load_best_model_at_end: true
metric_for_best_model: eval_loss
greater_is_better: false
dataloader_num_workers: 4
mixed_precision: fp16